{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fdfd5a-843f-4809-9ea9-24d0d89f7c5a",
   "metadata": {},
   "source": [
    "# Paper aquaculture IoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b92d23-7ee3-4d66-b8ce-fcf30264dcbd",
   "metadata": {},
   "source": [
    "1. Data Preparation:\n",
    "    •Gather all the papers related to IoT technologies in fisheries and aquaculture.\n",
    "    •Preprocess the text data by removing stop words, stemming, and lemmatization.\n",
    "    •Convert the text data into a format suitable for LDA analysis, such as a document-term matrix.\n",
    "2. LDA Model Training:\n",
    "•\tChoose the number of topics (clusters) you want the model to identify. This may require some experimentation.\n",
    "•\tTrain the LDA model on your preprocessed dataset.\n",
    "3. Explore Topics:\n",
    "•\tExamine the results of the LDA model to understand the distribution of topics across your papers.\n",
    "•\tIdentify the most significant words associated with each topic.\n",
    "4. Assign Topics to Papers:\n",
    "•\tAssign each paper to the topic that is most dominant in its content.\n",
    "•\tThis step helps in categorizing papers based on the identified themes.\n",
    "5. Visualize Results:\n",
    "•\tCreate visualizations to represent the results of your LDA analysis.\n",
    "•\tTools like word clouds, bar charts, or network graphs can help visualize the relationships between topics and words.\n",
    "6. Interpretation:\n",
    "•\tAnalyze the results to understand the main themes emerging from your dataset.\n",
    "•\tLook for patterns, connections, or trends within the identified topics.\n",
    "7. Refinement:\n",
    "•\tRefine your LDA model if needed. Adjust the number of topics or revisit the preprocessing steps based on the initial results.\n",
    "8. Write-Up:\n",
    "•\tDocument your findings and interpretations.\n",
    "•\tInclude visualizations and key insights derived from the LDA analysis.\n",
    "Tips:\n",
    "•\tExperiment with the Number of Topics: Try different numbers of topics to find the most meaningful and coherent grouping.\n",
    "•\tIterative Process: LDA analysis is often an iterative process. Refine parameters and preprocessing as needed.\n",
    "•\tValidate Results: Consider manually reviewing a subset of papers to validate the accuracy of the topic assignments.\n",
    "Tools:\n",
    "•\tPython libraries such as gensim or scikit-learn can be used for LDA analysis.\n",
    "•\tVisualization tools like pyLDAvis can assist in interpreting and presenting the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b06c111-10b6-4825-b8b4-5cecc2231d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycountry\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "     ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/10.1 MB 435.7 kB/s eta 0:00:24\n",
      "      --------------------------------------- 0.2/10.1 MB 2.0 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.3/10.1 MB 7.6 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 2.7/10.1 MB 13.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 4.4/10.1 MB 16.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 7.3/10.1 MB 22.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.1/10.1 MB 28.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.1/10.1 MB 28.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.1/10.1 MB 28.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.1/10.1 MB 28.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.1/10.1 MB 28.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.1/10.1 MB 18.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: setuptools in c:\\users\\kapoorab\\appdata\\local\\anaconda3\\lib\\site-packages (from pycountry) (68.0.0)\n",
      "Building wheels for collected packages: pycountry\n",
      "  Building wheel for pycountry (pyproject.toml): started\n",
      "  Building wheel for pycountry (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681895 sha256=9ee0f849e2b083d9e36cd4168b2b72560e7ec2f70077ffc67fb5c18aeb3570a9\n",
      "  Stored in directory: c:\\users\\kapoorab\\appdata\\local\\pip\\cache\\wheels\\cd\\29\\8b\\617685ed7942656b36efb06ff9247dbe832e3f4f7724fffc09\n",
      "Successfully built pycountry\n",
      "Installing collected packages: pycountry\n",
      "Successfully installed pycountry-22.3.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0404b44c-8b30-4296-b644-ee7c495693d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Obtaining dependency information for geopy from https://files.pythonhosted.org/packages/e5/15/cf2a69ade4b194aa524ac75112d5caac37414b20a3a03e6865dfe0bd1539/geopy-2.4.1-py3-none-any.whl.metadata\n",
      "  Downloading geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting geographiclib<3,>=1.52 (from geopy)\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "     ---------------------------------------- 0.0/40.3 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.3 kB ? eta -:--:--\n",
      "     ---------------------------- --------- 30.7/40.3 kB 435.7 kB/s eta 0:00:01\n",
      "     ---------------------------- --------- 30.7/40.3 kB 435.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 40.3/40.3 kB 273.5 kB/s eta 0:00:00\n",
      "Downloading geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "   ---------------------------------------- 0.0/125.4 kB ? eta -:--:--\n",
      "   ---------------------------------------  122.9/125.4 kB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 125.4/125.4 kB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5abfbf4b-b79b-4a14-b7df-b643cb065012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import metaknowledge as mk\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pandoc\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from geopy.geocoders import Nominatim\n",
    "#import pycountry\n",
    "import scipy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5adcc6-98fb-4570-94f9-e2bfc06ecde1",
   "metadata": {},
   "source": [
    "# Example of accessing nodes and edges in the co-citation network\n",
    "\n",
    "# Making a network of co-citations of journals\n",
    "coCiteJournals = RC.networkCoCitation(nodeType='journal', dropNonJournals=True)\n",
    "print(mk.graphStats(coCiteJournals))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37cc535a-72b1-45b7-8716-f505bc9e9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 8\n",
      "Edges: 9\n",
      "Isolates: 0\n",
      "Self loops: 1\n",
      "Density: 0.160714\n",
      "Transitivity: 0.142857\n",
      "Nodes: 334\n",
      "Edges: 881\n",
      "Isolates: 1\n",
      "Self loops: 0\n",
      "Density: 0.0158422\n",
      "Transitivity: 0.995147\n"
     ]
    }
   ],
   "source": [
    "# Making a citation network\n",
    "citationsA = RC.networkCitation(nodeType='year', keyWords=['A'], directed=True)\n",
    "print(mk.graphStats(citationsA))\n",
    "\n",
    "# Making a co-author network\n",
    "coAuths = RC.networkCoAuthor()\n",
    "print(mk.graphStats(coAuths))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Post-processing graphs\n",
    "minWeight = 3\n",
    "maxWeight = 10\n",
    "processedCoCiteJournals = mk.dropEdges(coCiteJournals, minWeight, maxWeight, dropSelfLoops=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1038bea-5dc8-4ac9-9fb8-69036fcd6ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 3152\n",
      "Edges: 126796\n",
      "Isolates: 0\n",
      "Self loops: 10\n",
      "Density: 0.0255329\n",
      "Transitivity: 0.701685\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import metaknowledge as mk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import metaknowledge.contour.plotting as mkv\n",
    "\n",
    "# Load a RecordCollection from a file\n",
    "RC = mk.RecordCollection('savedrecs (5).txt')\n",
    "\n",
    "# Making a co-citation network\n",
    "CoCitation = RC.networkCoCitation()\n",
    "\n",
    "# Visualize the co-citation network\n",
    "\n",
    "# Export the co-citation network\n",
    "mk.writeGraph(CoCitation, \"CoCitationNetwork\")\n",
    "\n",
    "# Read the exported graph back into Python\n",
    "ExportedCoCitation = mk.readGraph(\"CoCitationNetwork_edgeList.csv\", \"CoCitationNetwork_nodeAttributes.csv\")\n",
    "\n",
    "# Print the graph statistics\n",
    "print(mk.graphStats(ExportedCoCitation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8263486-7db8-40bc-829b-7109a62e6027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 stop words\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Read text from a TXT file\n",
    "file_path = 'savedrecs-5.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    sample_text = file.read()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "tokenize_sentence = sent_tokenize(sample_text)\n",
    "\n",
    "#print (tokenize_sentence)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# define the language for stopwords removal\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "print (\"\"\"{0} stop words\"\"\".format(len(stopwords)))\n",
    "\n",
    "tokenize_words = word_tokenize(sample_text)\n",
    "filtered_sample_text = [w for w in tokenize_words if not w in stopwords]\n",
    "\n",
    "# print ('\\nOriginal Text:')\n",
    "# print ('------------------\\n')\n",
    "# print (sample_text)\n",
    "# print ('\\n Filtered Text:')\n",
    "# print ('------------------\\n')\n",
    "# print (' '.join(str(token) for token in filtered_sample_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2d87d-0596-4e76-936f-034d54549fee",
   "metadata": {},
   "source": [
    "\n",
    "# Stop Words Removal\n",
    "Often, there are a few ubiquitous words which would appear to be of little value in helping the purpose of analysis but increases the dimensionality of feature set, are excluded from the vocabulary entirely as the part of stop words removal process. There are two considerations usually that motivate this removal.\n",
    "\n",
    "1. Irrelevance: Allows one to analyze only on content-bearing words. Stopwords, also called empty words because they generally do not bear much meaning, introduce noise in the analysis/modeling process\n",
    "2. Dimension: Removing the stopwords also allows one to reduce the tokens in documents significantly, and thereby decreasing feature dimension\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "Converting all characters into lowercase letters before stopwords removal process can introduce ambiguity in the text, and sometimes entirely changing the meaning of it. For example, with the expressions \"US citizen\" will be viewed as \"us citizen\" or \"IT scientist\" as \"it scientist\". Since both *us* and *it* are normally considered stop words, it would result in an inaccurate outcome. The strategy regarding the treatment of stopwords can thus be refined by identifying that \"US\" and \"IT\" are not pronouns in the above examples, through a part-of-speech tagging step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e1e8f-ec15-4351-8975-8c916862e4c3",
   "metadata": {},
   "source": [
    "Tokenization:\n",
    "\n",
    "It breaks down a paragraph into individual words.\n",
    "For example, \"I love coding\" becomes [\"I\", \"love\", \"coding\"].\n",
    "Stemming:\n",
    "\n",
    "It trims words to their base form.\n",
    "For instance, \"running\" becomes \"run\".\n",
    "It helps in simplifying words for analysis.\n",
    "Lemmatization:\n",
    "\n",
    "Similar to stemming but smarter.\n",
    "It reduces words to their essential form based on their meaning.\n",
    "For example, \"better\" becomes \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84e2e1dd-5589-460b-b4ff-012313ef89f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 3152\n",
      "Edges: 126796\n",
      "Isolates: 0\n",
      "Self loops: 10\n",
      "Density: 0.0255329\n",
      "Transitivity: 0.701685\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import metaknowledge as mk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import metaknowledge.contour.plotting as mkv\n",
    "\n",
    "# Load a RecordCollection from a file\n",
    "RC = mk.RecordCollection('savedrecs (5).txt')\n",
    "\n",
    "# Co-Citation Network\n",
    "CoCitation = RC.networkCoCitation()\n",
    "mk.writeGraph(CoCitation, \"CoCitationNetwork\")\n",
    "ExportedCoCitation = mk.readGraph(\"CoCitationNetwork_edgeList.csv\", \"CoCitationNetwork_nodeAttributes.csv\")\n",
    "print(mk.graphStats(ExportedCoCitation))\n",
    "\n",
    "# Citation Network\n",
    "Citation = RC.networkCitation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f77631eb-056d-4b00-aea7-42c3fb016785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the CSV file into a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1bde02-8358-40f5-aff9-3c13462afbae",
   "metadata": {},
   "source": [
    "Step 4: Prepare text for LDA analysis\n",
    "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by tokenizing the text and removing stopwords. Next, we convert the tokenized object into a corpus and dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c801dbe4-b0ea-4923-851f-343042b082ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Publication Type', 'Authors', 'Book Authors', 'Book Editors',\n",
       "       'Book Group Authors', 'Author Full Names', 'Book Author Full Names',\n",
       "       'Group Authors', 'Article Title', 'Source Title', 'Book Series Title',\n",
       "       'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title',\n",
       "       'Conference Date', 'Conference Location', 'Conference Sponsor',\n",
       "       'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract',\n",
       "       'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses',\n",
       "       'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred',\n",
       "       'Funding Text', 'Cited References', 'Cited Reference Count',\n",
       "       'Times Cited, WoS Core', 'Times Cited, All Databases',\n",
       "       '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher',\n",
       "       'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN',\n",
       "       'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date',\n",
       "       'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement',\n",
       "       'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page',\n",
       "       'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date',\n",
       "       'Number of Pages', 'WoS Categories', 'Web of Science Index',\n",
       "       'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations',\n",
       "       'Highly Cited Status', 'Hot Paper Status', 'Date of Export',\n",
       "       'UT (Unique WOS ID)', 'Web of Science Record'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the filtered dataset\n",
    "df_filtered = pd.read_csv(\"20231127WOSResults115.csv\")\n",
    "\n",
    "df_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d4eeae0-bd40-4241-b87e-8a2aaed82131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major Themes in Papers Published between 2000 and 2023:\n",
      "1. aquaponics: Occurrences = 33\n",
      "2. aquaponic: Occurrences = 26\n",
      "3. systems: Occurrences = 21\n",
      "4. system: Occurrences = 19\n",
      "5. production: Occurrences = 12\n",
      "6. fish: Occurrences = 10\n",
      "7. water: Occurrences = 9\n",
      "8. nitrogen: Occurrences = 9\n",
      "9. plant: Occurrences = 8\n",
      "10. lettuce: Occurrences = 8\n",
      "11. hydroponic: Occurrences = 8\n",
      "12. effects: Occurrences = 7\n",
      "13. commercial: Occurrences = 7\n",
      "14. growth: Occurrences = 7\n",
      "15. sustainability: Occurrences = 6\n",
      "16. recirculating: Occurrences = 5\n",
      "17. comparison: Occurrences = 5\n",
      "18. use: Occurrences = 5\n",
      "19. effect: Occurrences = 5\n",
      "20. hydroponics: Occurrences = 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Load the filtered dataset\n",
    "df_filtered = pd.read_csv(\"20231127WOSResults115.csv\")\n",
    "\n",
    "# Replace 'Publication Year' and 'Title' with the correct field names representing the publication year and title, respectively\n",
    "publication_year_field = 'Publication Year'\n",
    "title_field = 'Article Title'\n",
    "\n",
    "# Filter the dataset to include only the papers published in 2021\n",
    "\n",
    "# Filter the dataset to include only the papers from 2000 to 2023\n",
    "papers_published_between_2000_and_2023 = df_filtered[(df_filtered[publication_year_field] >= 2000) & (df_filtered[publication_year_field] <= 2023)]\n",
    "\n",
    "#papers_published_in_2021 = df_filtered[df_filtered[publication_year_field] == 2021]\n",
    "\n",
    "# Combine the 'Title' and 'Abstract' fields into a single text column (Optional: If 'Abstract' is available in the dataset)\n",
    "# papers_published_in_2021['Text'] = papers_published_in_2021[title_field] + \" \" + papers_published_in_2021['Abstract']\n",
    "\n",
    "# Tokenize the text and remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Keyword analysis on the titles\n",
    "keyword_occurrences = Counter()\n",
    "for title in papers_published_between_2000_and_2023[title_field]:\n",
    "    tokens = word_tokenize(title.lower())\n",
    "    keywords = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    keyword_occurrences.update(keywords)\n",
    "\n",
    "# Get the top 10 most frequent keywords as major themes\n",
    "major_themes = keyword_occurrences.most_common(20)\n",
    "\n",
    "# Print the major themes\n",
    "print(\"Major Themes in Papers Published between 2000 and 2023:\")\n",
    "for i, (keyword, occurrences) in enumerate(major_themes):\n",
    "    print(f\"{i+1}. {keyword}: Occurrences = {occurrences}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603b17e-097d-4492-8a28-6ad99d048601",
   "metadata": {},
   "source": [
    "## November 27, 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020afac0-6ea2-4f56-b9e8-70efa39db5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. J: Citation Count = 82\n",
      "2. C: Citation Count = 5\n"
     ]
    }
   ],
   "source": [
    "import metaknowledge as mk\n",
    "\n",
    "# Load the dataset of academic publications\n",
    "data = mk.RecordCollection(\"savedrecs (5).txt\")\n",
    "\n",
    "# Create a dictionary to store publication types and their citation counts\n",
    "publication_type_counts = {}\n",
    "\n",
    "# Calculate the citation counts for each publication type in the dataset\n",
    "for record in data:\n",
    "    publication_type = record['PT']\n",
    "    if publication_type in publication_type_counts:\n",
    "        publication_type_counts[publication_type] += 1\n",
    "    else:\n",
    "        publication_type_counts[publication_type] = 1\n",
    "\n",
    "# Filter the data to include only entries with 4 or more citations\n",
    "data_filtered = [record for record in data if publication_type_counts[record['PT']] >= 4]\n",
    "\n",
    "# Recalculate the citation counts for the filtered data\n",
    "publication_type_counts_filtered = {}\n",
    "\n",
    "for record in data_filtered:\n",
    "    publication_type = record['PT']\n",
    "    if publication_type in publication_type_counts_filtered:\n",
    "        publication_type_counts_filtered[publication_type] += 1\n",
    "    else:\n",
    "        publication_type_counts_filtered[publication_type] = 1\n",
    "\n",
    "# Sort the publication types based on their citation counts in descending order to get the most influential types\n",
    "sorted_publication_types = sorted(publication_type_counts_filtered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top influential publication types\n",
    "top_influential_publication_types = sorted_publication_types[:20]\n",
    "for i, (publication_type, citation_count) in enumerate(top_influential_publication_types):\n",
    "    print(f\"{i+1}. {publication_type}: Citation Count = {citation_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d105f6-1b15-4f3d-ae77-fe12001227e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. J: Citation Count = 74\n",
      "2. C: Citation Count = 5\n",
      "3. B: Citation Count = 1\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store publication types and their citation counts\n",
    "publication_type_counts = {}\n",
    "\n",
    "# Calculate the citation counts for each publication type in the dataset\n",
    "for record in data:\n",
    "    publication_type = record['PT']\n",
    "    if publication_type in publication_type_counts:\n",
    "        publication_type_counts[publication_type] += 1\n",
    "    else:\n",
    "        publication_type_counts[publication_type] = 1\n",
    "\n",
    "# Filter the data to include only entries with publication years between 2000 and 2022\n",
    "data_filtered = [record for record in data if 2000 <= int(record['PY']) <= 2022]\n",
    "\n",
    "# Recalculate the citation counts for the filtered data\n",
    "publication_type_counts_filtered = {}\n",
    "\n",
    "for record in data_filtered:\n",
    "    publication_type = record['PT']\n",
    "    if publication_type in publication_type_counts_filtered:\n",
    "        publication_type_counts_filtered[publication_type] += 1\n",
    "    else:\n",
    "        publication_type_counts_filtered[publication_type] = 1\n",
    "\n",
    "# Sort the publication types based on their citation counts in descending order to get the most influential types\n",
    "sorted_publication_types = sorted(publication_type_counts_filtered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top influential publication types\n",
    "top_influential_publication_types = sorted_publication_types[:20]\n",
    "for i, (publication_type, citation_count) in enumerate(top_influential_publication_types):\n",
    "    print(f\"{i+1}. {publication_type}: Citation Count = {citation_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657c6929-20b8-4041-abd8-f6858122719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 3152\n",
      "Edges: 126796\n",
      "Isolates: 0\n",
      "Self loops: 10\n",
      "Density: 0.0255329\n",
      "Transitivity: 0.701685\n",
      "Nodes: 557\n",
      "Edges: 14699\n",
      "Isolates: 0\n",
      "Self loops: 158\n",
      "Density: 0.0949266\n",
      "Transitivity: 0.412293\n",
      "Nodes: 83\n",
      "Edges: 554\n",
      "Isolates: 0\n",
      "Self loops: 9\n",
      "Density: 0.0813988\n",
      "Transitivity: 0.157455\n",
      "Nodes: 311\n",
      "Edges: 860\n",
      "Isolates: 2\n",
      "Self loops: 0\n",
      "Density: 0.0178405\n",
      "Transitivity: 0.836871\n",
      "Nodes: 37\n",
      "Edges: 51\n",
      "Isolates: 0\n",
      "Self loops: 0\n",
      "Density: 0.0765766\n",
      "Transitivity: 0.81203\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import metaknowledge as mk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import metaknowledge.contour.plotting as mkv\n",
    "\n",
    "# Load a RecordCollection from a file\n",
    "RC = mk.RecordCollection('savedrecs (5).txt')\n",
    "\n",
    "# Create a co-citation network\n",
    "coCites = RC.networkCoCitation()\n",
    "print(mk.graphStats(coCites, makeString=True))\n",
    "\n",
    "# Create a co-citation network focusing on journals\n",
    "coCiteJournals = RC.networkCoCitation(nodeType='journal', dropNonJournals=True)\n",
    "print(mk.graphStats(coCiteJournals))\n",
    "\n",
    "# Visualize the co-citation journal network using spring layout\n",
    "# nx.draw_spring(coCiteJournals)\n",
    "\n",
    "# Create a citation network based on keywords\n",
    "citationsA = RC.networkCitation(nodeType='year', keyWords=['aquaculture', 'technology', 'aquaponics', 'IoT'])\n",
    "print(mk.graphStats(citationsA))\n",
    "\n",
    "# Create a co-author network\n",
    "coAuths = RC.networkCoAuthor()\n",
    "print(mk.graphStats(coAuths))\n",
    "\n",
    "# Post-process the co-author network\n",
    "minWeight = 2\n",
    "maxWeight = 10\n",
    "mk.dropEdges(coAuths, minWeight, maxWeight, dropSelfLoops=True)\n",
    "mk.dropNodesByDegree(coAuths, 1)\n",
    "\n",
    "# Visualize the processed co-author network\n",
    "# nx.draw_spring(coAuths)\n",
    "\n",
    "# Export the graph to files\n",
    "mk.writeGraph(coAuths, \"FinalJournalCoCites\")\n",
    "\n",
    "# Read the graph back into Python\n",
    "FinalJournalCoCites = mk.readGraph(\"FinalJournalCoCites_edgeList.csv\", \"FinalJournalCoCites_nodeAttributes.csv\")\n",
    "print(mk.graphStats(FinalJournalCoCites))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae66241-4ebf-45f7-9a67-820cf42e347e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Literature on Drones and their use in fisheries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "168123c8-7150-479a-b4ea-75d8426bcf79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment: 0.6588\n",
      "Positive sentiment: 0.594\n",
      "Negative sentiment: 0.0\n",
      "Neutral sentiment: 0.406\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the text that you want to analyze.\n",
    "text = \"This is a great book!\"\n",
    "\n",
    "# Create a ToneAnalyzer object.\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze the text.\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the results.\n",
    "print(\"Overall sentiment:\", scores['compound'])\n",
    "print(\"Positive sentiment:\", scores['pos'])\n",
    "print(\"Negative sentiment:\", scores['neg'])\n",
    "print(\"Neutral sentiment:\", scores['neu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8433908-3a3c-4e8b-865c-2044bb4249de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Book Authors</th>\n",
       "      <th>Book Editors</th>\n",
       "      <th>Book Group Authors</th>\n",
       "      <th>Author Full Names</th>\n",
       "      <th>Book Author Full Names</th>\n",
       "      <th>Group Authors</th>\n",
       "      <th>Article Title</th>\n",
       "      <th>Source Title</th>\n",
       "      <th>...</th>\n",
       "      <th>Web of Science Index</th>\n",
       "      <th>Research Areas</th>\n",
       "      <th>IDS Number</th>\n",
       "      <th>Pubmed Id</th>\n",
       "      <th>Open Access Designations</th>\n",
       "      <th>Highly Cited Status</th>\n",
       "      <th>Hot Paper Status</th>\n",
       "      <th>Date of Export</th>\n",
       "      <th>UT (Unique WOS ID)</th>\n",
       "      <th>Web of Science Record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J</td>\n",
       "      <td>Provost, EJ; Butcher, PA; Coleman, MA; Kelaher...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provost, Euan J.; Butcher, Paul A.; Coleman, M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the viability of small aerial drones...</td>\n",
       "      <td>FISHERIES MANAGEMENT AND ECOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>OP0OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000557866800001</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J</td>\n",
       "      <td>Kopaska, J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kopaska, Jeff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drones-A Fisheries Assessment Tool?</td>\n",
       "      <td>FISHERIES</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>AM8HY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000340115400009</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J</td>\n",
       "      <td>Provost, EJ; Butcher, PA; Coleman, MA; Bloom, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provost, Euan J.; Butcher, Paul A.; Coleman, M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aerial drone technology can assist compliance ...</td>\n",
       "      <td>FISHERIES MANAGEMENT AND ECOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)...</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>ME7WU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000544866100008</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J</td>\n",
       "      <td>Bloom, D; Butcher, PA; Colefax, AP; Provost, E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bloom, Daniel; Butcher, Paul A.; Colefax, Andr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drones detect illegal and derelict crab traps ...</td>\n",
       "      <td>FISHERIES MANAGEMENT AND ECOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)...</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>IL0SO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000477010400001</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J</td>\n",
       "      <td>Winkler, AC; Butler, EC; Attwood, CG; Mann, BQ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Winkler, Alexander C.; Butler, Edward C.; Attw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The emergence of marine recreational drone fis...</td>\n",
       "      <td>AMBIO</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)...</td>\n",
       "      <td>Engineering; Environmental Sciences &amp; Ecology</td>\n",
       "      <td>YP0LB</td>\n",
       "      <td>34145559.0</td>\n",
       "      <td>Green Published</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000663256000002</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Publication Type                                            Authors  \\\n",
       "0                J  Provost, EJ; Butcher, PA; Coleman, MA; Kelaher...   \n",
       "1                J                                         Kopaska, J   \n",
       "2                J  Provost, EJ; Butcher, PA; Coleman, MA; Bloom, ...   \n",
       "3                J  Bloom, D; Butcher, PA; Colefax, AP; Provost, E...   \n",
       "4                J  Winkler, AC; Butler, EC; Attwood, CG; Mann, BQ...   \n",
       "\n",
       "   Book Authors  Book Editors Book Group Authors  \\\n",
       "0           NaN           NaN                NaN   \n",
       "1           NaN           NaN                NaN   \n",
       "2           NaN           NaN                NaN   \n",
       "3           NaN           NaN                NaN   \n",
       "4           NaN           NaN                NaN   \n",
       "\n",
       "                                   Author Full Names  Book Author Full Names  \\\n",
       "0  Provost, Euan J.; Butcher, Paul A.; Coleman, M...                     NaN   \n",
       "1                                      Kopaska, Jeff                     NaN   \n",
       "2  Provost, Euan J.; Butcher, Paul A.; Coleman, M...                     NaN   \n",
       "3  Bloom, Daniel; Butcher, Paul A.; Colefax, Andr...                     NaN   \n",
       "4  Winkler, Alexander C.; Butler, Edward C.; Attw...                     NaN   \n",
       "\n",
       "   Group Authors                                      Article Title  \\\n",
       "0            NaN  Assessing the viability of small aerial drones...   \n",
       "1            NaN                Drones-A Fisheries Assessment Tool?   \n",
       "2            NaN  Aerial drone technology can assist compliance ...   \n",
       "3            NaN  Drones detect illegal and derelict crab traps ...   \n",
       "4            NaN  The emergence of marine recreational drone fis...   \n",
       "\n",
       "                       Source Title  ...  \\\n",
       "0  FISHERIES MANAGEMENT AND ECOLOGY  ...   \n",
       "1                         FISHERIES  ...   \n",
       "2  FISHERIES MANAGEMENT AND ECOLOGY  ...   \n",
       "3  FISHERIES MANAGEMENT AND ECOLOGY  ...   \n",
       "4                             AMBIO  ...   \n",
       "\n",
       "                                Web of Science Index  \\\n",
       "0     Science Citation Index Expanded (SCI-EXPANDED)   \n",
       "1     Science Citation Index Expanded (SCI-EXPANDED)   \n",
       "2  Science Citation Index Expanded (SCI-EXPANDED)...   \n",
       "3  Science Citation Index Expanded (SCI-EXPANDED)...   \n",
       "4  Science Citation Index Expanded (SCI-EXPANDED)...   \n",
       "\n",
       "                                  Research Areas IDS Number   Pubmed Id  \\\n",
       "0                                      Fisheries      OP0OK         NaN   \n",
       "1                                      Fisheries      AM8HY         NaN   \n",
       "2                                      Fisheries      ME7WU         NaN   \n",
       "3                                      Fisheries      IL0SO         NaN   \n",
       "4  Engineering; Environmental Sciences & Ecology      YP0LB  34145559.0   \n",
       "\n",
       "  Open Access Designations Highly Cited Status Hot Paper Status  \\\n",
       "0                      NaN                 NaN              NaN   \n",
       "1                      NaN                 NaN              NaN   \n",
       "2                      NaN                 NaN              NaN   \n",
       "3                      NaN                 NaN              NaN   \n",
       "4          Green Published                 NaN              NaN   \n",
       "\n",
       "  Date of Export   UT (Unique WOS ID)               Web of Science Record  \n",
       "0     2023-11-27  WOS:000557866800001  View Full Record in Web of Science  \n",
       "1     2023-11-27  WOS:000340115400009  View Full Record in Web of Science  \n",
       "2     2023-11-27  WOS:000544866100008  View Full Record in Web of Science  \n",
       "3     2023-11-27  WOS:000477010400001  View Full Record in Web of Science  \n",
       "4     2023-11-27  WOS:000663256000002  View Full Record in Web of Science  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers= pd.read_csv('droneLitWoS56.csv', encoding='latin1')\n",
    "#Add code to process text columns together\n",
    "papers.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ba279-9f57-45e4-9280-7c42f9a55154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
