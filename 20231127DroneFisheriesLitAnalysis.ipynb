{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fdfd5a-843f-4809-9ea9-24d0d89f7c5a",
   "metadata": {},
   "source": [
    "# Paper aquaculture IoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b92d23-7ee3-4d66-b8ce-fcf30264dcbd",
   "metadata": {},
   "source": [
    "1. Data Preparation:\n",
    "    •Gather all the papers related to IoT technologies in fisheries and aquaculture.\n",
    "    •Preprocess the text data by removing stop words, stemming, and lemmatization.\n",
    "    •Convert the text data into a format suitable for LDA analysis, such as a document-term matrix.\n",
    "2. LDA Model Training:\n",
    "•\tChoose the number of topics (clusters) you want the model to identify. This may require some experimentation.\n",
    "•\tTrain the LDA model on your preprocessed dataset.\n",
    "3. Explore Topics:\n",
    "•\tExamine the results of the LDA model to understand the distribution of topics across your papers.\n",
    "•\tIdentify the most significant words associated with each topic.\n",
    "4. Assign Topics to Papers:\n",
    "•\tAssign each paper to the topic that is most dominant in its content.\n",
    "•\tThis step helps in categorizing papers based on the identified themes.\n",
    "5. Visualize Results:\n",
    "•\tCreate visualizations to represent the results of your LDA analysis.\n",
    "•\tTools like word clouds, bar charts, or network graphs can help visualize the relationships between topics and words.\n",
    "6. Interpretation:\n",
    "•\tAnalyze the results to understand the main themes emerging from your dataset.\n",
    "•\tLook for patterns, connections, or trends within the identified topics.\n",
    "7. Refinement:\n",
    "•\tRefine your LDA model if needed. Adjust the number of topics or revisit the preprocessing steps based on the initial results.\n",
    "8. Write-Up:\n",
    "•\tDocument your findings and interpretations.\n",
    "•\tInclude visualizations and key insights derived from the LDA analysis.\n",
    "Tips:\n",
    "•\tExperiment with the Number of Topics: Try different numbers of topics to find the most meaningful and coherent grouping.\n",
    "•\tIterative Process: LDA analysis is often an iterative process. Refine parameters and preprocessing as needed.\n",
    "•\tValidate Results: Consider manually reviewing a subset of papers to validate the accuracy of the topic assignments.\n",
    "Tools:\n",
    "•\tPython libraries such as gensim or scikit-learn can be used for LDA analysis.\n",
    "•\tVisualization tools like pyLDAvis can assist in interpreting and presenting the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0404b44c-8b30-4296-b644-ee7c495693d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "BadInputFile",
     "evalue": "'/Users/rishi/OneDrive - Michigan State University/20231116PaperAquaculture/CoCitationNetwork_nodeAttributes.csv' does not match any known file type, but has the requested extension 'csv'. Its header might be damaged or it could have been modified by another program.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownFile\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/metaknowledge/recordCollection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inCollection, name, extension, cached, quietStart)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0mrecordType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecordHandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                                 \u001b[0;32mif\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                                     \u001b[0mrecordTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecordType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/metaknowledge/fileHandlers.py\u001b[0m in \u001b[0;36munrecognizedFileHandler\u001b[0;34m(fileName)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munrecognizedFileHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mUnknownFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'{}' is not recognized my metaknowledge.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownFile\u001b[0m: '/Users/rishi/OneDrive - Michigan State University/20231116PaperAquaculture/CoCitationNetwork_nodeAttributes.csv' is not recognized my metaknowledge.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadInputFile\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ly/rgjn5pn521xds5tj0m76ys_40000gn/T/ipykernel_86625/461053524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecordCollection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/metaknowledge/recordCollection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inCollection, name, extension, cached, quietStart)\u001b[0m\n\u001b[1;32m    145\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mUnknownFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                                 \u001b[0;32mraise\u001b[0m \u001b[0mBadInputFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'{}' does not match any known file type, but has the requested extension '{}'. Its header might be damaged or it could have been modified by another program.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadInputFile\u001b[0m: '/Users/rishi/OneDrive - Michigan State University/20231116PaperAquaculture/CoCitationNetwork_nodeAttributes.csv' does not match any known file type, but has the requested extension 'csv'. Its header might be damaged or it could have been modified by another program."
     ]
    }
   ],
   "source": [
    "RC = mk.RecordCollection(\"/Users/rishi/OneDrive - Michigan State University/\", extension = \"csv\")\n",
    "repr(RC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5abfbf4b-b79b-4a14-b7df-b643cb065012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import metaknowledge as mk\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pandoc\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from geopy.geocoders import Nominatim\n",
    "#import pycountry\n",
    "import scipy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b7cd81-331d-4f72-839f-b719aca87988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<metaknowledge.RecordCollection object files-from-/Users/rishi/OneDrive - Michigan State University/20231116PaperAquaculture>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Record Collections\n",
    "RC = mk.RecordCollection(\".\")\n",
    "repr(RC)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4263d3c7-7662-4f08-a532-7dab8dc18ec0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ly/rgjn5pn521xds5tj0m76ys_40000gn/T/ipykernel_86625/3039088143.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Making a network of co-citations of journals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoCiteJournals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworkCoCitation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodeType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'journal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropNonJournals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoCiteJournals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RC' is not defined"
     ]
    }
   ],
   "source": [
    "# Making a network of co-citations of journals\n",
    "coCiteJournals = RC.networkCoCitation(nodeType='journal', dropNonJournals=True)\n",
    "print(mk.graphStats(coCiteJournals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37cc535a-72b1-45b7-8716-f505bc9e9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 8\n",
      "Edges: 9\n",
      "Isolates: 0\n",
      "Self loops: 1\n",
      "Density: 0.160714\n",
      "Transitivity: 0.142857\n",
      "Nodes: 334\n",
      "Edges: 881\n",
      "Isolates: 1\n",
      "Self loops: 0\n",
      "Density: 0.0158422\n",
      "Transitivity: 0.995147\n"
     ]
    }
   ],
   "source": [
    "# Making a citation network\n",
    "citationsA = RC.networkCitation(nodeType='year', keyWords=['A'], directed=True)\n",
    "print(mk.graphStats(citationsA))\n",
    "\n",
    "# Making a co-author network\n",
    "coAuths = RC.networkCoAuthor()\n",
    "print(mk.graphStats(coAuths))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Post-processing graphs\n",
    "minWeight = 3\n",
    "maxWeight = 10\n",
    "processedCoCiteJournals = mk.dropEdges(coCiteJournals, minWeight, maxWeight, dropSelfLoops=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1038bea-5dc8-4ac9-9fb8-69036fcd6ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 3152\n",
      "Edges: 126796\n",
      "Isolates: 0\n",
      "Self loops: 10\n",
      "Density: 0.0255329\n",
      "Transitivity: 0.701685\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import metaknowledge as mk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import metaknowledge.contour.plotting as mkv\n",
    "\n",
    "# Load a RecordCollection from a file\n",
    "RC = mk.RecordCollection('savedrecs (5).txt')\n",
    "\n",
    "# Making a co-citation network\n",
    "CoCitation = RC.networkCoCitation()\n",
    "\n",
    "# Visualize the co-citation network\n",
    "\n",
    "# Export the co-citation network\n",
    "mk.writeGraph(CoCitation, \"CoCitationNetwork\")\n",
    "\n",
    "# Read the exported graph back into Python\n",
    "ExportedCoCitation = mk.readGraph(\"CoCitationNetwork_edgeList.csv\", \"CoCitationNetwork_nodeAttributes.csv\")\n",
    "\n",
    "# Print the graph statistics\n",
    "print(mk.graphStats(ExportedCoCitation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8263486-7db8-40bc-829b-7109a62e6027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/rishi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 stop words\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Read text from a TXT file\n",
    "file_path = 'savedrecs-5.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    sample_text = file.read()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "tokenize_sentence = sent_tokenize(sample_text)\n",
    "\n",
    "#print (tokenize_sentence)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# define the language for stopwords removal\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "print (\"\"\"{0} stop words\"\"\".format(len(stopwords)))\n",
    "\n",
    "tokenize_words = word_tokenize(sample_text)\n",
    "filtered_sample_text = [w for w in tokenize_words if not w in stopwords]\n",
    "\n",
    "# print ('\\nOriginal Text:')\n",
    "# print ('------------------\\n')\n",
    "# print (sample_text)\n",
    "# print ('\\n Filtered Text:')\n",
    "# print ('------------------\\n')\n",
    "# print (' '.join(str(token) for token in filtered_sample_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2d87d-0596-4e76-936f-034d54549fee",
   "metadata": {},
   "source": [
    "\n",
    "## Stop Words Removal\n",
    "Often, there are a few ubiquitous words which would appear to be of little value in helping the purpose of analysis but increases the dimensionality of feature set, are excluded from the vocabulary entirely as the part of stop words removal process. There are two considerations usually that motivate this removal.\n",
    "\n",
    "1. Irrelevance: Allows one to analyze only on content-bearing words. Stopwords, also called empty words because they generally do not bear much meaning, introduce noise in the analysis/modeling process\n",
    "2. Dimension: Removing the stopwords also allows one to reduce the tokens in documents significantly, and thereby decreasing feature dimension\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "Converting all characters into lowercase letters before stopwords removal process can introduce ambiguity in the text, and sometimes entirely changing the meaning of it. For example, with the expressions \"US citizen\" will be viewed as \"us citizen\" or \"IT scientist\" as \"it scientist\". Since both *us* and *it* are normally considered stop words, it would result in an inaccurate outcome. The strategy regarding the treatment of stopwords can thus be refined by identifying that \"US\" and \"IT\" are not pronouns in the above examples, through a part-of-speech tagging step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e39230d7-5692-4eda-b4d2-fb4b96b95f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenize_words = word_tokenize(sample_text)\n",
    "\n",
    "stemmed_sample_text = []\n",
    "for token in tokenize_words:\n",
    "    stemmed_sample_text.append(ps.stem(token))\n",
    "\n",
    "lemma_sample_text = []\n",
    "for token in tokenize_words:\n",
    "    lemma_sample_text.append(lemmatizer.lemmatize(token))\n",
    "    \n",
    "# print ('\\nOriginal Text:')\n",
    "# print ('------------------\\n')\n",
    "# print (sample_text)\n",
    "\n",
    "# print ('\\nFiltered Text: Stemming')\n",
    "# print ('------------------\\n')\n",
    "# print (' '.join(str(token) for token in stemmed_sample_text))\n",
    "\n",
    "# print ('\\nFiltered Text: Lemmatization')\n",
    "# print ('--------------------------------\\n')\n",
    "# print (' '.join(str(token) for token in lemma_sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "704f449b-197c-434f-9dfb-e37743900eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(sample_text)\n",
    "\n",
    "# print(\"Original Tokens:\")\n",
    "# print(tokens)\n",
    "# print('---------------------------------------------------------\\n')\n",
    "\n",
    "# Stemming using Porter Stemmer\n",
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "\n",
    "# print(\"Stemmed Tokens:\")\n",
    "# print(stemmed_tokens)\n",
    "# print('---------------------------------------------------------\\n')\n",
    "\n",
    "# Lemmatization using WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# print(\"Lemmatized Tokens:\")\n",
    "# print(lemmatized_tokens)\n",
    "# print('---------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e1e8f-ec15-4351-8975-8c916862e4c3",
   "metadata": {},
   "source": [
    "Tokenization:\n",
    "\n",
    "It breaks down a paragraph into individual words.\n",
    "For example, \"I love coding\" becomes [\"I\", \"love\", \"coding\"].\n",
    "Stemming:\n",
    "\n",
    "It trims words to their base form.\n",
    "For instance, \"running\" becomes \"run\".\n",
    "It helps in simplifying words for analysis.\n",
    "Lemmatization:\n",
    "\n",
    "Similar to stemming but smarter.\n",
    "It reduces words to their essential form based on their meaning.\n",
    "For example, \"better\" becomes \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84e2e1dd-5589-460b-b4ff-012313ef89f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 3152\n",
      "Edges: 126796\n",
      "Isolates: 0\n",
      "Self loops: 10\n",
      "Density: 0.0255329\n",
      "Transitivity: 0.701685\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import metaknowledge as mk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import metaknowledge.contour.plotting as mkv\n",
    "\n",
    "# Load a RecordCollection from a file\n",
    "RC = mk.RecordCollection('savedrecs (5).txt')\n",
    "\n",
    "# Co-Citation Network\n",
    "CoCitation = RC.networkCoCitation()\n",
    "mk.writeGraph(CoCitation, \"CoCitationNetwork\")\n",
    "ExportedCoCitation = mk.readGraph(\"CoCitationNetwork_edgeList.csv\", \"CoCitationNetwork_nodeAttributes.csv\")\n",
    "print(mk.graphStats(ExportedCoCitation))\n",
    "\n",
    "# Citation Network\n",
    "Citation = RC.networkCitation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77631eb-056d-4b00-aea7-42c3fb016785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the CSV file into a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e499087e-5131-45ec-b963-4cf6681091e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Book Authors</th>\n",
       "      <th>Book Editors</th>\n",
       "      <th>Book Group Authors</th>\n",
       "      <th>Author Full Names</th>\n",
       "      <th>Book Author Full Names</th>\n",
       "      <th>Group Authors</th>\n",
       "      <th>Article Title</th>\n",
       "      <th>Source Title</th>\n",
       "      <th>...</th>\n",
       "      <th>Web of Science Index</th>\n",
       "      <th>Research Areas</th>\n",
       "      <th>IDS Number</th>\n",
       "      <th>Pubmed Id</th>\n",
       "      <th>Open Access Designations</th>\n",
       "      <th>Highly Cited Status</th>\n",
       "      <th>Hot Paper Status</th>\n",
       "      <th>Date of Export</th>\n",
       "      <th>UT (Unique WOS ID)</th>\n",
       "      <th>Web of Science Record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J</td>\n",
       "      <td>Provost, EJ; Butcher, PA; Coleman, MA; Kelaher...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provost, Euan J.; Butcher, Paul A.; Coleman, M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the viability of small aerial drones...</td>\n",
       "      <td>FISHERIES MANAGEMENT AND ECOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>OP0OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000557866800001</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J</td>\n",
       "      <td>Kopaska, J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kopaska, Jeff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drones-A Fisheries Assessment Tool?</td>\n",
       "      <td>FISHERIES</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>AM8HY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000340115400009</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J</td>\n",
       "      <td>Provost, EJ; Butcher, PA; Coleman, MA; Bloom, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provost, Euan J.; Butcher, Paul A.; Coleman, M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aerial drone technology can assist compliance ...</td>\n",
       "      <td>FISHERIES MANAGEMENT AND ECOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)...</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>ME7WU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000544866100008</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J</td>\n",
       "      <td>Bloom, D; Butcher, PA; Colefax, AP; Provost, E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bloom, Daniel; Butcher, Paul A.; Colefax, Andr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drones detect illegal and derelict crab traps ...</td>\n",
       "      <td>FISHERIES MANAGEMENT AND ECOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)...</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>IL0SO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000477010400001</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J</td>\n",
       "      <td>Winkler, AC; Butler, EC; Attwood, CG; Mann, BQ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Winkler, Alexander C.; Butler, Edward C.; Attw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The emergence of marine recreational drone fis...</td>\n",
       "      <td>AMBIO</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)...</td>\n",
       "      <td>Engineering; Environmental Sciences &amp; Ecology</td>\n",
       "      <td>YP0LB</td>\n",
       "      <td>34145559.0</td>\n",
       "      <td>Green Published</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000663256000002</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Publication Type                                            Authors  \\\n",
       "0                J  Provost, EJ; Butcher, PA; Coleman, MA; Kelaher...   \n",
       "1                J                                         Kopaska, J   \n",
       "2                J  Provost, EJ; Butcher, PA; Coleman, MA; Bloom, ...   \n",
       "3                J  Bloom, D; Butcher, PA; Colefax, AP; Provost, E...   \n",
       "4                J  Winkler, AC; Butler, EC; Attwood, CG; Mann, BQ...   \n",
       "\n",
       "   Book Authors  Book Editors Book Group Authors  \\\n",
       "0           NaN           NaN                NaN   \n",
       "1           NaN           NaN                NaN   \n",
       "2           NaN           NaN                NaN   \n",
       "3           NaN           NaN                NaN   \n",
       "4           NaN           NaN                NaN   \n",
       "\n",
       "                                   Author Full Names  Book Author Full Names  \\\n",
       "0  Provost, Euan J.; Butcher, Paul A.; Coleman, M...                     NaN   \n",
       "1                                      Kopaska, Jeff                     NaN   \n",
       "2  Provost, Euan J.; Butcher, Paul A.; Coleman, M...                     NaN   \n",
       "3  Bloom, Daniel; Butcher, Paul A.; Colefax, Andr...                     NaN   \n",
       "4  Winkler, Alexander C.; Butler, Edward C.; Attw...                     NaN   \n",
       "\n",
       "   Group Authors                                      Article Title  \\\n",
       "0            NaN  Assessing the viability of small aerial drones...   \n",
       "1            NaN                Drones-A Fisheries Assessment Tool?   \n",
       "2            NaN  Aerial drone technology can assist compliance ...   \n",
       "3            NaN  Drones detect illegal and derelict crab traps ...   \n",
       "4            NaN  The emergence of marine recreational drone fis...   \n",
       "\n",
       "                       Source Title  ...  \\\n",
       "0  FISHERIES MANAGEMENT AND ECOLOGY  ...   \n",
       "1                         FISHERIES  ...   \n",
       "2  FISHERIES MANAGEMENT AND ECOLOGY  ...   \n",
       "3  FISHERIES MANAGEMENT AND ECOLOGY  ...   \n",
       "4                             AMBIO  ...   \n",
       "\n",
       "                                Web of Science Index  \\\n",
       "0     Science Citation Index Expanded (SCI-EXPANDED)   \n",
       "1     Science Citation Index Expanded (SCI-EXPANDED)   \n",
       "2  Science Citation Index Expanded (SCI-EXPANDED)...   \n",
       "3  Science Citation Index Expanded (SCI-EXPANDED)...   \n",
       "4  Science Citation Index Expanded (SCI-EXPANDED)...   \n",
       "\n",
       "                                  Research Areas IDS Number   Pubmed Id  \\\n",
       "0                                      Fisheries      OP0OK         NaN   \n",
       "1                                      Fisheries      AM8HY         NaN   \n",
       "2                                      Fisheries      ME7WU         NaN   \n",
       "3                                      Fisheries      IL0SO         NaN   \n",
       "4  Engineering; Environmental Sciences & Ecology      YP0LB  34145559.0   \n",
       "\n",
       "  Open Access Designations Highly Cited Status Hot Paper Status  \\\n",
       "0                      NaN                 NaN              NaN   \n",
       "1                      NaN                 NaN              NaN   \n",
       "2                      NaN                 NaN              NaN   \n",
       "3                      NaN                 NaN              NaN   \n",
       "4          Green Published                 NaN              NaN   \n",
       "\n",
       "  Date of Export   UT (Unique WOS ID)               Web of Science Record  \n",
       "0     2023-11-27  WOS:000557866800001  View Full Record in Web of Science  \n",
       "1     2023-11-27  WOS:000340115400009  View Full Record in Web of Science  \n",
       "2     2023-11-27  WOS:000544866100008  View Full Record in Web of Science  \n",
       "3     2023-11-27  WOS:000477010400001  View Full Record in Web of Science  \n",
       "4     2023-11-27  WOS:000663256000002  View Full Record in Web of Science  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers= pd.read_csv('droneLitWoS56.csv', encoding='latin1')\n",
    "\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738493d4-e074-4550-b71a-bdce34ee37f0",
   "metadata": {},
   "source": [
    "\n",
    "Step 2: Data Cleaning\n",
    "Since the goal of this analysis is to perform topic modeling, let's focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we'll only look at 100 papers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "507dd89b-976b-458f-b0cd-6aa9184295a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Publication Type', 'Authors', 'Book Authors', 'Book Editors',\n",
       "       'Book Group Authors', 'Author Full Names', 'Book Author Full Names',\n",
       "       'Group Authors', 'Article Title', 'Source Title', 'Book Series Title',\n",
       "       'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title',\n",
       "       'Conference Date', 'Conference Location', 'Conference Sponsor',\n",
       "       'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract',\n",
       "       'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses',\n",
       "       'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred',\n",
       "       'Funding Text', 'Cited References', 'Cited Reference Count',\n",
       "       'Times Cited, WoS Core', 'Times Cited, All Databases',\n",
       "       '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher',\n",
       "       'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN',\n",
       "       'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date',\n",
       "       'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement',\n",
       "       'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page',\n",
       "       'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date',\n",
       "       'Number of Pages', 'WoS Categories', 'Web of Science Index',\n",
       "       'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations',\n",
       "       'Highly Cited Status', 'Hot Paper Status', 'Date of Export',\n",
       "       'UT (Unique WOS ID)', 'Web of Science Record'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71574256-c17c-4980-b6da-d83fa49c6ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Article Title', 'Publication Date', 'Publication Year', 'Abstract',\n",
       "       'Author Keywords', 'Keywords Plus', 'WoS Categories',\n",
       "       'Cited References', 'Cited Reference Count', 'Times Cited, WoS Core',\n",
       "       'Times Cited, All Databases', '180 Day Usage Count',\n",
       "       'Since 2013 Usage Count', 'Funding Orgs', 'Funding Name Preferred',\n",
       "       'Funding Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns to keep\n",
    "columns_to_keep = [\n",
    "    'Article Title', 'Publication Date',\n",
    "       'Publication Year','Abstract', 'Author Keywords', 'Keywords Plus','WoS Categories',\n",
    "    'Cited References', 'Cited Reference Count',\n",
    "    'Times Cited, WoS Core', 'Times Cited, All Databases',\n",
    "    '180 Day Usage Count', 'Since 2013 Usage Count',\n",
    "    'Funding Orgs', 'Funding Name Preferred', 'Funding Text'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to keep only the specified columns\n",
    "papers_selected_columns = papers[columns_to_keep]\n",
    "papers_selected_columns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af855a3a-362f-4c12-b4a1-617bac51fe38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Book Authors</th>\n",
       "      <th>Book Editors</th>\n",
       "      <th>Book Group Authors</th>\n",
       "      <th>Author Full Names</th>\n",
       "      <th>Book Author Full Names</th>\n",
       "      <th>Group Authors</th>\n",
       "      <th>Article Title</th>\n",
       "      <th>Source Title</th>\n",
       "      <th>...</th>\n",
       "      <th>Web of Science Index</th>\n",
       "      <th>Research Areas</th>\n",
       "      <th>IDS Number</th>\n",
       "      <th>Pubmed Id</th>\n",
       "      <th>Open Access Designations</th>\n",
       "      <th>Highly Cited Status</th>\n",
       "      <th>Hot Paper Status</th>\n",
       "      <th>Date of Export</th>\n",
       "      <th>UT (Unique WOS ID)</th>\n",
       "      <th>Web of Science Record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J</td>\n",
       "      <td>Provost, EJ; Butcher, PA; Coleman, MA; Kelaher...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provost, Euan J.; Butcher, Paul A.; Coleman, M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the viability of small aerial drones...</td>\n",
       "      <td>FISHERIES MANAGEMENT AND ECOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>OP0OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000557866800001</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J</td>\n",
       "      <td>Kopaska, J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kopaska, Jeff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drones-A Fisheries Assessment Tool?</td>\n",
       "      <td>FISHERIES</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>AM8HY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000340115400009</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J</td>\n",
       "      <td>Provost, EJ; Butcher, PA; Coleman, MA; Bloom, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provost, Euan J.; Butcher, Paul A.; Coleman, M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aerial drone technology can assist compliance ...</td>\n",
       "      <td>FISHERIES MANAGEMENT AND ECOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)...</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>ME7WU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000544866100008</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J</td>\n",
       "      <td>Bloom, D; Butcher, PA; Colefax, AP; Provost, E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bloom, Daniel; Butcher, Paul A.; Colefax, Andr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drones detect illegal and derelict crab traps ...</td>\n",
       "      <td>FISHERIES MANAGEMENT AND ECOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)...</td>\n",
       "      <td>Fisheries</td>\n",
       "      <td>IL0SO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000477010400001</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J</td>\n",
       "      <td>Winkler, AC; Butler, EC; Attwood, CG; Mann, BQ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Winkler, Alexander C.; Butler, Edward C.; Attw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The emergence of marine recreational drone fis...</td>\n",
       "      <td>AMBIO</td>\n",
       "      <td>...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)...</td>\n",
       "      <td>Engineering; Environmental Sciences &amp; Ecology</td>\n",
       "      <td>YP0LB</td>\n",
       "      <td>34145559.0</td>\n",
       "      <td>Green Published</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>WOS:000663256000002</td>\n",
       "      <td>View Full Record in Web of Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Publication Type                                            Authors  \\\n",
       "0                J  Provost, EJ; Butcher, PA; Coleman, MA; Kelaher...   \n",
       "1                J                                         Kopaska, J   \n",
       "2                J  Provost, EJ; Butcher, PA; Coleman, MA; Bloom, ...   \n",
       "3                J  Bloom, D; Butcher, PA; Colefax, AP; Provost, E...   \n",
       "4                J  Winkler, AC; Butler, EC; Attwood, CG; Mann, BQ...   \n",
       "\n",
       "   Book Authors  Book Editors Book Group Authors  \\\n",
       "0           NaN           NaN                NaN   \n",
       "1           NaN           NaN                NaN   \n",
       "2           NaN           NaN                NaN   \n",
       "3           NaN           NaN                NaN   \n",
       "4           NaN           NaN                NaN   \n",
       "\n",
       "                                   Author Full Names  Book Author Full Names  \\\n",
       "0  Provost, Euan J.; Butcher, Paul A.; Coleman, M...                     NaN   \n",
       "1                                      Kopaska, Jeff                     NaN   \n",
       "2  Provost, Euan J.; Butcher, Paul A.; Coleman, M...                     NaN   \n",
       "3  Bloom, Daniel; Butcher, Paul A.; Colefax, Andr...                     NaN   \n",
       "4  Winkler, Alexander C.; Butler, Edward C.; Attw...                     NaN   \n",
       "\n",
       "   Group Authors                                      Article Title  \\\n",
       "0            NaN  Assessing the viability of small aerial drones...   \n",
       "1            NaN                Drones-A Fisheries Assessment Tool?   \n",
       "2            NaN  Aerial drone technology can assist compliance ...   \n",
       "3            NaN  Drones detect illegal and derelict crab traps ...   \n",
       "4            NaN  The emergence of marine recreational drone fis...   \n",
       "\n",
       "                       Source Title  ...  \\\n",
       "0  FISHERIES MANAGEMENT AND ECOLOGY  ...   \n",
       "1                         FISHERIES  ...   \n",
       "2  FISHERIES MANAGEMENT AND ECOLOGY  ...   \n",
       "3  FISHERIES MANAGEMENT AND ECOLOGY  ...   \n",
       "4                             AMBIO  ...   \n",
       "\n",
       "                                Web of Science Index  \\\n",
       "0     Science Citation Index Expanded (SCI-EXPANDED)   \n",
       "1     Science Citation Index Expanded (SCI-EXPANDED)   \n",
       "2  Science Citation Index Expanded (SCI-EXPANDED)...   \n",
       "3  Science Citation Index Expanded (SCI-EXPANDED)...   \n",
       "4  Science Citation Index Expanded (SCI-EXPANDED)...   \n",
       "\n",
       "                                  Research Areas IDS Number   Pubmed Id  \\\n",
       "0                                      Fisheries      OP0OK         NaN   \n",
       "1                                      Fisheries      AM8HY         NaN   \n",
       "2                                      Fisheries      ME7WU         NaN   \n",
       "3                                      Fisheries      IL0SO         NaN   \n",
       "4  Engineering; Environmental Sciences & Ecology      YP0LB  34145559.0   \n",
       "\n",
       "  Open Access Designations Highly Cited Status Hot Paper Status  \\\n",
       "0                      NaN                 NaN              NaN   \n",
       "1                      NaN                 NaN              NaN   \n",
       "2                      NaN                 NaN              NaN   \n",
       "3                      NaN                 NaN              NaN   \n",
       "4          Green Published                 NaN              NaN   \n",
       "\n",
       "  Date of Export   UT (Unique WOS ID)               Web of Science Record  \n",
       "0     2023-11-27  WOS:000557866800001  View Full Record in Web of Science  \n",
       "1     2023-11-27  WOS:000340115400009  View Full Record in Web of Science  \n",
       "2     2023-11-27  WOS:000544866100008  View Full Record in Web of Science  \n",
       "3     2023-11-27  WOS:000477010400001  View Full Record in Web of Science  \n",
       "4     2023-11-27  WOS:000663256000002  View Full Record in Web of Science  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Print out the first rows of papers\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3897d-32d6-4bd1-9f25-f6c379a3b95a",
   "metadata": {},
   "source": [
    "Remove punctuation/lower casing\n",
    "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3980f3eb-dd89-47d9-9a04-0e620cf407d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kapoorab\\AppData\\Local\\Temp\\ipykernel_7740\\2959890390.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  papers_selected_columns['Abstract'] = papers_selected_columns['Abstract'].replace({np.nan: '', None: ''})\n",
      "C:\\Users\\kapoorab\\AppData\\Local\\Temp\\ipykernel_7740\\2959890390.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  papers_selected_columns['paper_text_processed'] = papers_selected_columns['Abstract'].map(lambda x: re.sub('[,\\.!?]', '', str(x)))\n",
      "C:\\Users\\kapoorab\\AppData\\Local\\Temp\\ipykernel_7740\\2959890390.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  papers_selected_columns['paper_text_processed'] = papers_selected_columns['paper_text_processed'].map(lambda x: x.lower())\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m papers_selected_columns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_text_processed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m papers_selected_columns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_text_processed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Import the necessary libraries for word cloud and visualization\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a WordCloud object\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd  # Make sure to import pandas if not done previously\n",
    "\n",
    "# Check for NaN or non-string values and replace with an empty string\n",
    "papers_selected_columns['Abstract'] = papers_selected_columns['Abstract'].replace({np.nan: '', None: ''})\n",
    "\n",
    "# Preprocess the 'paper_text_processed' column\n",
    "papers_selected_columns['paper_text_processed'] = papers_selected_columns['Abstract'].map(lambda x: re.sub('[,\\.!?]', '', str(x)))\n",
    "papers_selected_columns['paper_text_processed'] = papers_selected_columns['paper_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Import the necessary libraries for word cloud and visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(papers_selected_columns['paper_text_processed'].values))\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1bde02-8358-40f5-aff9-3c13462afbae",
   "metadata": {},
   "source": [
    "Step 4: Prepare text for LDA analysis\n",
    "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by tokenizing the text and removing stopwords. Next, we convert the tokenized object into a corpus and dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c801dbe4-b0ea-4923-851f-343042b082ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Publication Type', 'Authors', 'Book Authors', 'Book Editors',\n",
       "       'Book Group Authors', 'Author Full Names', 'Book Author Full Names',\n",
       "       'Group Authors', 'Article Title', 'Source Title', 'Book Series Title',\n",
       "       'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title',\n",
       "       'Conference Date', 'Conference Location', 'Conference Sponsor',\n",
       "       'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract',\n",
       "       'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses',\n",
       "       'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred',\n",
       "       'Funding Text', 'Cited References', 'Cited Reference Count',\n",
       "       'Times Cited, WoS Core', 'Times Cited, All Databases',\n",
       "       '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher',\n",
       "       'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN',\n",
       "       'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date',\n",
       "       'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement',\n",
       "       'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page',\n",
       "       'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date',\n",
       "       'Number of Pages', 'WoS Categories', 'Web of Science Index',\n",
       "       'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations',\n",
       "       'Highly Cited Status', 'Hot Paper Status', 'Date of Export',\n",
       "       'UT (Unique WOS ID)', 'Web of Science Record'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the filtered dataset\n",
    "df_filtered = pd.read_csv(\"20231127WOSResults115.csv\")\n",
    "\n",
    "df_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d4eeae0-bd40-4241-b87e-8a2aaed82131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major Themes in Papers Published between 2000 and 2023:\n",
      "1. aquaponics: Occurrences = 33\n",
      "2. aquaponic: Occurrences = 26\n",
      "3. systems: Occurrences = 21\n",
      "4. system: Occurrences = 19\n",
      "5. production: Occurrences = 12\n",
      "6. fish: Occurrences = 10\n",
      "7. water: Occurrences = 9\n",
      "8. nitrogen: Occurrences = 9\n",
      "9. plant: Occurrences = 8\n",
      "10. lettuce: Occurrences = 8\n",
      "11. hydroponic: Occurrences = 8\n",
      "12. effects: Occurrences = 7\n",
      "13. commercial: Occurrences = 7\n",
      "14. growth: Occurrences = 7\n",
      "15. sustainability: Occurrences = 6\n",
      "16. recirculating: Occurrences = 5\n",
      "17. comparison: Occurrences = 5\n",
      "18. use: Occurrences = 5\n",
      "19. effect: Occurrences = 5\n",
      "20. hydroponics: Occurrences = 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Load the filtered dataset\n",
    "df_filtered = pd.read_csv(\"20231127WOSResults115.csv\")\n",
    "\n",
    "# Replace 'Publication Year' and 'Title' with the correct field names representing the publication year and title, respectively\n",
    "publication_year_field = 'Publication Year'\n",
    "title_field = 'Article Title'\n",
    "\n",
    "# Filter the dataset to include only the papers published in 2021\n",
    "\n",
    "# Filter the dataset to include only the papers from 2000 to 2023\n",
    "papers_published_between_2000_and_2023 = df_filtered[(df_filtered[publication_year_field] >= 2000) & (df_filtered[publication_year_field] <= 2023)]\n",
    "\n",
    "#papers_published_in_2021 = df_filtered[df_filtered[publication_year_field] == 2021]\n",
    "\n",
    "# Combine the 'Title' and 'Abstract' fields into a single text column (Optional: If 'Abstract' is available in the dataset)\n",
    "# papers_published_in_2021['Text'] = papers_published_in_2021[title_field] + \" \" + papers_published_in_2021['Abstract']\n",
    "\n",
    "# Tokenize the text and remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Keyword analysis on the titles\n",
    "keyword_occurrences = Counter()\n",
    "for title in papers_published_between_2000_and_2023[title_field]:\n",
    "    tokens = word_tokenize(title.lower())\n",
    "    keywords = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    keyword_occurrences.update(keywords)\n",
    "\n",
    "# Get the top 10 most frequent keywords as major themes\n",
    "major_themes = keyword_occurrences.most_common(20)\n",
    "\n",
    "# Print the major themes\n",
    "print(\"Major Themes in Papers Published between 2000 and 2023:\")\n",
    "for i, (keyword, occurrences) in enumerate(major_themes):\n",
    "    print(f\"{i+1}. {keyword}: Occurrences = {occurrences}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603b17e-097d-4492-8a28-6ad99d048601",
   "metadata": {},
   "source": [
    "## November 27, 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "020afac0-6ea2-4f56-b9e8-70efa39db5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. J: Citation Count = 82\n",
      "2. C: Citation Count = 5\n"
     ]
    }
   ],
   "source": [
    "import metaknowledge as mk\n",
    "\n",
    "# Load the dataset of academic publications\n",
    "data = mk.RecordCollection(\"savedrecs (5).txt\")\n",
    "\n",
    "# Create a dictionary to store publication types and their citation counts\n",
    "publication_type_counts = {}\n",
    "\n",
    "# Calculate the citation counts for each publication type in the dataset\n",
    "for record in data:\n",
    "    publication_type = record['PT']\n",
    "    if publication_type in publication_type_counts:\n",
    "        publication_type_counts[publication_type] += 1\n",
    "    else:\n",
    "        publication_type_counts[publication_type] = 1\n",
    "\n",
    "# Filter the data to include only entries with 4 or more citations\n",
    "data_filtered = [record for record in data if publication_type_counts[record['PT']] >= 4]\n",
    "\n",
    "# Recalculate the citation counts for the filtered data\n",
    "publication_type_counts_filtered = {}\n",
    "\n",
    "for record in data_filtered:\n",
    "    publication_type = record['PT']\n",
    "    if publication_type in publication_type_counts_filtered:\n",
    "        publication_type_counts_filtered[publication_type] += 1\n",
    "    else:\n",
    "        publication_type_counts_filtered[publication_type] = 1\n",
    "\n",
    "# Sort the publication types based on their citation counts in descending order to get the most influential types\n",
    "sorted_publication_types = sorted(publication_type_counts_filtered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top influential publication types\n",
    "top_influential_publication_types = sorted_publication_types[:20]\n",
    "for i, (publication_type, citation_count) in enumerate(top_influential_publication_types):\n",
    "    print(f\"{i+1}. {publication_type}: Citation Count = {citation_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d105f6-1b15-4f3d-ae77-fe12001227e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. J: Citation Count = 74\n",
      "2. C: Citation Count = 5\n",
      "3. B: Citation Count = 1\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store publication types and their citation counts\n",
    "publication_type_counts = {}\n",
    "\n",
    "# Calculate the citation counts for each publication type in the dataset\n",
    "for record in data:\n",
    "    publication_type = record['PT']\n",
    "    if publication_type in publication_type_counts:\n",
    "        publication_type_counts[publication_type] += 1\n",
    "    else:\n",
    "        publication_type_counts[publication_type] = 1\n",
    "\n",
    "# Filter the data to include only entries with publication years between 2000 and 2022\n",
    "data_filtered = [record for record in data if 2000 <= int(record['PY']) <= 2022]\n",
    "\n",
    "# Recalculate the citation counts for the filtered data\n",
    "publication_type_counts_filtered = {}\n",
    "\n",
    "for record in data_filtered:\n",
    "    publication_type = record['PT']\n",
    "    if publication_type in publication_type_counts_filtered:\n",
    "        publication_type_counts_filtered[publication_type] += 1\n",
    "    else:\n",
    "        publication_type_counts_filtered[publication_type] = 1\n",
    "\n",
    "# Sort the publication types based on their citation counts in descending order to get the most influential types\n",
    "sorted_publication_types = sorted(publication_type_counts_filtered.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top influential publication types\n",
    "top_influential_publication_types = sorted_publication_types[:20]\n",
    "for i, (publication_type, citation_count) in enumerate(top_influential_publication_types):\n",
    "    print(f\"{i+1}. {publication_type}: Citation Count = {citation_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657c6929-20b8-4041-abd8-f6858122719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 3152\n",
      "Edges: 126796\n",
      "Isolates: 0\n",
      "Self loops: 10\n",
      "Density: 0.0255329\n",
      "Transitivity: 0.701685\n",
      "Nodes: 557\n",
      "Edges: 14699\n",
      "Isolates: 0\n",
      "Self loops: 158\n",
      "Density: 0.0949266\n",
      "Transitivity: 0.412293\n",
      "Nodes: 83\n",
      "Edges: 554\n",
      "Isolates: 0\n",
      "Self loops: 9\n",
      "Density: 0.0813988\n",
      "Transitivity: 0.157455\n",
      "Nodes: 311\n",
      "Edges: 860\n",
      "Isolates: 2\n",
      "Self loops: 0\n",
      "Density: 0.0178405\n",
      "Transitivity: 0.836871\n",
      "Nodes: 37\n",
      "Edges: 51\n",
      "Isolates: 0\n",
      "Self loops: 0\n",
      "Density: 0.0765766\n",
      "Transitivity: 0.81203\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import metaknowledge as mk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import metaknowledge.contour.plotting as mkv\n",
    "\n",
    "# Load a RecordCollection from a file\n",
    "RC = mk.RecordCollection('savedrecs (5).txt')\n",
    "\n",
    "# Create a co-citation network\n",
    "coCites = RC.networkCoCitation()\n",
    "print(mk.graphStats(coCites, makeString=True))\n",
    "\n",
    "# Create a co-citation network focusing on journals\n",
    "coCiteJournals = RC.networkCoCitation(nodeType='journal', dropNonJournals=True)\n",
    "print(mk.graphStats(coCiteJournals))\n",
    "\n",
    "# Visualize the co-citation journal network using spring layout\n",
    "# nx.draw_spring(coCiteJournals)\n",
    "\n",
    "# Create a citation network based on keywords\n",
    "citationsA = RC.networkCitation(nodeType='year', keyWords=['aquaculture', 'technology', 'aquaponics', 'IoT'])\n",
    "print(mk.graphStats(citationsA))\n",
    "\n",
    "# Create a co-author network\n",
    "coAuths = RC.networkCoAuthor()\n",
    "print(mk.graphStats(coAuths))\n",
    "\n",
    "# Post-process the co-author network\n",
    "minWeight = 2\n",
    "maxWeight = 10\n",
    "mk.dropEdges(coAuths, minWeight, maxWeight, dropSelfLoops=True)\n",
    "mk.dropNodesByDegree(coAuths, 1)\n",
    "\n",
    "# Visualize the processed co-author network\n",
    "# nx.draw_spring(coAuths)\n",
    "\n",
    "# Export the graph to files\n",
    "mk.writeGraph(coAuths, \"FinalJournalCoCites\")\n",
    "\n",
    "# Read the graph back into Python\n",
    "FinalJournalCoCites = mk.readGraph(\"FinalJournalCoCites_edgeList.csv\", \"FinalJournalCoCites_nodeAttributes.csv\")\n",
    "print(mk.graphStats(FinalJournalCoCites))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae66241-4ebf-45f7-9a67-820cf42e347e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Literature on Drones and their use in fisheries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168123c8-7150-479a-b4ea-75d8426bcf79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment: 0.6588\n",
      "Positive sentiment: 0.594\n",
      "Negative sentiment: 0.0\n",
      "Neutral sentiment: 0.406\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the text that you want to analyze.\n",
    "text = \"This is a great book!\"\n",
    "\n",
    "# Create a ToneAnalyzer object.\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze the text.\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "# Print the results.\n",
    "print(\"Overall sentiment:\", scores['compound'])\n",
    "print(\"Positive sentiment:\", scores['pos'])\n",
    "print(\"Negative sentiment:\", scores['neg'])\n",
    "print(\"Neutral sentiment:\", scores['neu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7838da-1ca1-49be-a6a8-958db4ba16fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
